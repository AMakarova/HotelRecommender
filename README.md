**Content-Based Hotel Recommender System Based on Embeddings Optimised to Predict Customer Purchasing Behaviours**

Traditionally, recommendation systems fall into two groups with their own strengths and weaknesses - content-based systems and collaborative filtering systems.

Content-based systems are based on the assumption that customers will like products that are similar to the ones they have originally purchased. The similarity may be calculated based on one or multiple features and relies on domain expertise that allows to identify features that are important to customers. These systems are most useful where little data exists on historical customer purchases.

Collaborative filtering systems are domain agnostic and do not rely on expert knowledge, as they only utilise customer ratings (or purchasing behaviour) as input features. This group of algorithms tends to surpass content-based approaches in performance where data is sufficient, as recommendations are drawn based on real-world customer choices. Collaborative filtering systems can utilise a variety of mathematical algorithms, but lately neural network architectures such as Deep Boltzmann Machines have started to increase in popularity. The most critial weakness of collaborative filtering systems is the cold-start problem, as both customers and products that haven't been seen by the model previously cannot be paired up. Furthermore, large and sparse datasets present challenges for this group of algorithms.

A range of approaches exist to combine content-based and collaborative filtering systems to utilise all available information and leverage the strength of both algorithms. Typically, this is done via a range of hybrid algorithms that combine outputs of content-based and collaborative filtering systems.

In this project I propose a different approach, where content-based recommender system is reframed as a supervised task, where user behaviour data (e.g., cosine simialrity between distinct products) is used as labels. Feature sets for 2 lists of products are fed in a deep neural network in parallel and transformed through a series of hidden dense layers, while cosine similarities that are calculated for combinations of products on both lists are used as labels. The final layer of the model calculates cosine similarity between the right and left streams. This forces the model to learn a set of useful embeddings for each product based on available raw features. This architecture allows to utilise both structured and unstructured product meta information (e.g., for hotels these could be geographic coordinates, star ratings, concepts, as well as natural language descriptions, images etc.)

The resulting embeddings then allow to map each product into multi-dimensional space (which can be further flattened with algorithms such as t-SNE). Storing a set of embeddings corresponding to each product is significantly more compact than storing similarity matrix between all possible pairs of products (especially as the number of products increases) and can be used directly for model deployment which eliminates the need to do real-time model scoring and replaces it with a dictionary lookup and cosine similarity calculation.

As opposed to Deep Boltzmann Machine architecture (or similar), embeddings are not learned based on (and therefore are not dependent on) the unique product identifiers, but rather on set of features that describe the product. Therefore, any new product can be mapped directly into the embedding vector even before collecting any user behaviour information and without having to retrain the model, which resolves the cold start problem. Similarly, the model is forced to generalise the information and is not sensitive to infrequent accidental overlaps (e.g., where each product is purchased by a low number of customers and bahavioral data in unreliable), which allows to cope well with sparse datasets. The model can be effectively trained on a comparatively small subset of products (where co-purchacing is more dense) and generalise to the rest of the products. As a further benefit, a set of embeddings serves as an 'interface' between diverse datasets. For example, a different model could be trained to map customer characteristics (e.g., demographics, lifetime spend, purchasing habits) into embeddings obtained in the original model, which would allow to implicitly handle issues like temporal decay and cold start.

Please note that all personally identifiable customer information was removed from the datasets and data is not included with this notebook, however this still falls under TUI IP and should not be shared further without permission.
